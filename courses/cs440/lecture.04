#!/bin/bash
;`
#####################################################################
# Notes
#####################################################################
;`
[NOTES]
notes: Chapter 3: Solving Problems by Searching
    #################################################################
    # Review
    #################################################################
    Agents and Rationality
    PEAS:
        Performance Measure
        Environment
        Acutators
        Sensors
    """Agent""": An agent is anything that can be viewed as percieving its environment through sensors and acting upon that environment through actuators.
    Review Environment Types
        F   --> Fully observable   vs.   Partially Observable
        E   --> Episodic           vs.   Sequential
        K   --> Known              vs.   Unknown
        D   --> Discrete           vs.   Continuous
        D   --> Deterministic      vs.   Stochastic
        A-S --> Single-Agent       vs.   Multi-Agent
        S   --> Static             vs.   Dynamic
        """F.E.K.D.D.A.S.S."""
    #################################################################
    # Preview
    #################################################################
    Part 1:
        Deterministic environments:
            search
            constraint satifaction
            logic
        Multi-Agent, strategic environments:
            minimax search
            games
            >>> Can also be stochastic and partially observable
    Part 2:
        Stochastic environments:
            Episodic:
                """Bayesian Networks"""
                """Pattern Classifiers"""
            Sequential, Known:
                """Markov Decision Processes"""
            Sequential, Unknown:
                """Reinforcement Learning"""
    #################################################################
    # Chapter 3: Solving Problems By Searching
    #################################################################
    Types of Agents
        """Reflex Agent"""
            consider how the world IS
            choose action based on current percept (and maybe memory or a model of the world''s current state).
            do not consider the furture consequences of the actions
            Ex: Bacteria, Republicans
        """Planning Agent"""
            consider how the world WOULD BE
            decisions based on (hypothesized) consequences of actions
            must have a model on how the world responds to actions
            must formulate goal
    #################################################################
    # Search
    #################################################################
    We will consider the problem of designing """goal-based agents""" in fully observable, deterministic, discrete, known environments.
        E.G. A MAZE
        From "start state" to """goal state"""
            The Agent must find a """sequence of actions""" that reaches the goal.
            The performance measure is defined by:
                (a) Reaching the goal and
                (b) how "expensive" the path to the goal is
            We are focused on the process of finding the solution. While executing the solution, we assume the agent can safely ignore its percepts ("""open-loop system""").
    #################################################################
    # Search Problem Components
    #################################################################
    Components:
        State specification
            """Initial state"""
        Actions
        Transition Model
            What state results from performing a given action in a given state?
        """Goal State"""
        """Path Cost"""
            Assume that it is a sum of "non-negative" """step costs""".
    The """optimal solution""" is the sequence of actions that gives the lowest path cost for reaching the goal.
    #################################################################
    # Search Example: Romania
    #################################################################
    Given:
        You are on a vaction in Romani.
        You are currently in the city of Arad.
        Your flight leaves tomorrow out of Bucharest.
    Initial State:
        Arad
    Actions:
        Drive from city A to city B
    Transition Model:
        If you go from city A to city B, you end up in city B.
    Goal State:
        Bucharest
    Path Cost:
        Kilometers driven i.e. sum of edge cost
    #################################################################
    # State Space
    #################################################################
    """State Space"""
        The initial state, actions, and transition model define the state space of a problem.
            The set of all states reachable from the initial state by any sequence of actions.
            Can be represented as a """directed graph""" where the nodes are states and the links between nodes are actions.
    What is the state space for the Romania problem?
        The set of all cities in the graph
        The set of all actions that can be taken at each city
        !!!Set of all nodes and their connections
        The set is definitely fininte
        THE MAP ITSELF could be considered the state space
    #################################################################
    # Search Example: Vacuum World
    #################################################################
    States:
        Cell A or B
        Whether or not each cell has dirt
        Total of 8 states
        "What if ther are n possible locations?"
            n * 2**n states, where n is the number of cells in the environment
            The (2**n) is the number of combinations of dirty/clean cells possible in the environment, whereas the (n) represents ever location that the vacuum could be in at any given time.
            !!! The size of the state space grows exponentially with the size of the world.
    Actions:
        Move Left/Right
        Suck
    Transition Model:
        if in <certain cell>:
            left = <valid/invalid>
            right = <valid/invalid>
            suck = <valid/invalid>
    >>> Here, Dr. Lazebnik shows an image of the vacuum world """state space graph"""
    #################################################################
    # Search Example: The 8-Puzzle
    #################################################################
    start state = \
    [
        [7,   2,   4],
        [5, null,  6],
        [8,   3,   1]
    ]
    end state = \
    [
        [null,1,   2],
        [3,   4,   5],
        [6,   7,   8]
    ]
    States:
        Set of all possible sequences on the board.
        9!(factorial) states? Not quite.
        For an 8-puzzle: 181,440 states=(9!/2)
            Divided by 2 in order to excluede illegal moves, like lifting a piece up and out of the puzzle to then place it in a different spot.
                Mathematically proving this necessity is non-trivial, but all we have to know is that it is true.
        For a 15-puzzle:
            approx. 10 trillion states
        For a 24-puzzle:
            approx. 10**25 states
        ??? What if there is more than one null square?
    Actions:
        Slide a number into a blank location
        or
        Move "blank" left, up, right, or down
    Path Cost:
        The number of slides required to move from the start state to the end state.
        1 per slide.
    >>> Finding the optimal solution of an n-puzzle is """NP-hard"""
    #################################################################
    # Search Example: Robot Motion Planning
    #################################################################
    States:
        Real-valued joint parameters such as angles and displacements
    Actions:
        Continuous motions of robot joints
    Goal State:
        Configuration in which the robot is able to "grasp" the object
    Path Cost:
        The time taken to execute the action, the smoothness of the path, etc.
    #################################################################
    # Search, Continued
    #################################################################
    Givens:
        T -->   """Transition Model"""
        I -->   """Initial State"""
        G -->   """Goal State"""
        P -->   """Path Cost"""
        A -->   """Actions"""
        """T.I.G.P.A."""
    How do we find the optimal solution?
        How about building a state space and then using """Dijkstra's Shortest Path Algorithm"""?
            !!!Complexity of Dijkstra is O(E + V log V)
                V --> the number of vertices
                E --> the number of edges
                E and V comprise the state space
            In practice, this is very hard to do. Instead of building the entire graph, build the graph incrementally
            Prune impossible/obviously non-optimal solutions
    #################################################################
    # Search: The Basic Idea
    #################################################################
    Let us begin at the start state and EXPAND it by making a list of al possible """successor states""".
    Maintain a """Frontier""", or a list of unexpanded states
    At each step, pick a state from the frontier to EXPAND
    Keep going until you reach the goal state ///do
    Try to expand as few states as possible.
    >>> A tradeoff exists between the optimality of solution and the time taken to compute it.
    #################################################################
    # Search Tree
    #################################################################
    "What if" tree of sequences of actions and outcomes
    Root node corresponds to the starting state
    The children of a node correspond to the successor states of that node''s state
    A path through the tree coresponds to a sequence of actions
        A solution is a path ending in the goal state
    !!! FIRST ASSIGNMENT IS TO BUILD A SEARCH TREE
    Nodes vs. States:
        A state is a representation of the world, while a node is a data structure that is part of a search tree. ///do
            A node has to keep track of the pointer to its parent, the path cost, and possibly other info.
        IN A NUTSHELL: Nodes hold data about a given state in addition to other information.
    The farther down the tree you can calculate, the smarter the AI
    #################################################################
    # The Search Algorithm Outline
    #################################################################
    !!! Following pseudocode to be implemented in the assignment
    :::
    Initialize the """frontier""" using the """starting state"""
    while frontier is not empty do
        choose a frontier node according to a """search strategy""" and remove it from the frontier
        if the node contains the """goal state""", return solution
        else EXPAND the node and add its children to the frontier
    :::
    >>> Possible recursion solution?
    Handling Repeated States:
        Every time you EXPAND a node, add that state to the """explored set""", and do not put explored states on the frontier again
        Every time you add a node to the frontier, check whether it already exists in the frontier with a higher path cost, and if yes, replace that node with the new one.

;`
#####################################################################
# Vocabulary
#####################################################################
;`
[VOCABULARY]

;`
#####################################################################
# Timeline
#####################################################################
;`
[TIMELINE]

